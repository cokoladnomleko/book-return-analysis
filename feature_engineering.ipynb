{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from geo_utils import *\n",
    "from time import sleep\n",
    "from geopy.distance import geodesic\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = joblib.load('data/customers_clean.pkl')\n",
    "checkouts = joblib.load('data/checkouts_clean.pkl')\n",
    "books = joblib.load('data/books_clean.pkl')\n",
    "libraries = joblib.load('data/libraries_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance-based features\n",
    "---\n",
    "\n",
    "* Features based on library location data are added, since analysis showed differences in return rates\n",
    "* Additional location data is gathered using openstreetmap, Overpass and geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = get_public_transport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(x, stations):\n",
    "    sleep(3) # avoiding rate limits\n",
    "    data = find_nearest_stations(x, stations)\n",
    "    return data['min_distance'], data['num_near']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "libraries[['city', 'region']] = libraries[['city', 'region']].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = libraries.apply((lambda x: fetch_data(f\"{x['street_address']}, {x['region']}\", stations)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tmp_df)):\n",
    "    libraries.loc[i, 'closest_transport'] = tmp_df[i][0]\n",
    "    libraries.loc[i, 'num_close_transport'] = tmp_df[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = checkouts.merge(books, on='id')\\\n",
    "            .merge(customers, left_on='patron_id', right_on='id', suffixes=('', '_customers'))\\\n",
    "            .merge(libraries, left_on='library_id', right_on='id', suffixes=('', '_libraries'))\n",
    "\n",
    "df_merged['book_age'] = (df_merged['date_checkout']-df_merged['publishedDate']).dt.days/365.25\n",
    "df_merged.loc[df_merged['book_age']<0, 'book_age'] = df_merged.loc[df_merged['book_age']>0, 'book_age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_deadline = 28\n",
    "\n",
    "df_merged['days_return'] = (df_merged['date_returned']-df_merged['date_checkout']).dt.days\n",
    "df_merged['target'] = df_merged['days_return']>return_deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = [\n",
    "    'id', 'patron_id', 'library_id', 'id_customers', 'id_libraries', # ids not useful for modelling\n",
    "    'title', 'authors', 'name', 'publisher', 'name_libraries',\n",
    "    'street_address', 'street_address_libraries',\n",
    "    'date_checkout', 'date_returned', 'birth_date', 'publishedDate',\n",
    "    'gender'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged.drop(columns=cols_to_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns will be encoded using one-hot-encoding. Another option would be to use ordinal encoding for columns where values have some hierarchy (e.g. education level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Cleaning & Feature Selection\n",
    "\n",
    "Features which are constant, or have too low variance can be removed.\n",
    "\n",
    "Also, features that are correlated with one another (in this case we would keep the one with higher correlation to target). Mutual information can be used instead of correlation to catch non-linear relationships.\n",
    "\n",
    "Since tree-based methods are most likely to be used in the modelling exercise here, this step will be skipped as they are less prone to being affected by useless features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is split between train and test and saved on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(columns=['target']), df['target'], test_size=0.2, stratify=df['target'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/y_test.pkl']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(x_train, 'data/x_train.pkl')\n",
    "joblib.dump(x_test, 'data/x_test.pkl')\n",
    "joblib.dump(y_train, 'data/y_train.pkl')\n",
    "joblib.dump(y_test, 'data/y_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
